# Release Note for V 1.0.1-cap7
- Probably this is not how a release note should be. just to take notes on whats is going on.
- Using Web-Flux and WebClient we can develop non-blocking APIs synchronous sending request
  and response without blocking any thread.
- We can use reactive support for MongoDB, Spring Web-Flux, WebClient, and Spring Data rely on Spring Reactor
- When we must use blocking code, for example, when using Spring Data for JPA, we can encapsulate 
  the processing of the blocking code by scheduling the processing of it in a dedicated thread pool.
- The main idea in this release is learn how Spring Cloud Stream can be used to switch between different brokers
in this case RabbitMQ and Kafka without any change in the code. It just requires few changes in docker compose files
- We use the new API actuator/health to check our services and test it 

## Added Feature:
1. Event Driven interfaces to sent CREATE and DELETE petitions.
2. Async. interfaces for fetching data using Web-Flux
3. Docker Compose to run all system using RabbitMQ system.
4. Actuator/health check for all microservice [ Upgrade to SpringBoot 2.4.5]
5. Spring Upgrade to Spring Boot 2.5.1 and Spring Cloud 2020.0.3
6. support to start either RabbitMQ [docker-compose.yml] or Kafka [docker-compose-kafka.yml]

## Setup
### Manual Test
1. clone project
2. build project:
    ````shell
    ./gradlew build
    ````
3. Run docker compose RabbitMQ.
   
   3.1. run the application using RabbitMQ:
    ````shell
    docker-compose build && docker-compose up
    ````
    3.2. run the application using RabbitMQ partitions:
    ````shell
    export COMPOSE_FILE=docker-compose-partitons.yml
    docker-compose build && docker-compose up
    ````
    3.3. check or monitoring RabbitMQ http://localhost:15672/  user:guest pass:guest
4. Run the application using Kafka:
    ````shell
    export COMPOSE_FILE=docker-compose-kafka.yml
    docker-compose build && docker-compose up
    # list all container created
    docker ps 
    ````
    4.1 checking all kafka topics were created:
    ````shell
    docker exec kafka /opt/kafka/bin/kafka-topics.sh --describe --zookeeper zookeeper --topic products
    docker exec kafka /opt/kafka/bin/kafka-topics.sh --describe --zookeeper zookeeper --topic recommendations
    docker exec kafka /opt/kafka/bin/kafka-topics.sh --describe --zookeeper zookeeper --topic reviews
    ````
   4.2. Checking the messages pushed. It will get outputs after executing step 5:
   ````shell
   # products
   # recommendations
   # reviews
   docker-compose exec kafka /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic products --from-beginning --timeout-ms 1000
   # or from a specific partition:
   docker-compose exec kafka /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic products --from-beginning --timeout-ms 1000 --partition 1
    ````
5. Testing out the application:
    ````shell
    cd product-recomendation
    curl -X POST localhost:8080/product-composite -H "Content-Type: application/json" -d @product-data.json |jq .
    ````
6. Verifying the new product was created accordingly:
    ````shell
    curl localhost:8080/product-composite/2 -s | jq .
    ````
7. Stopping all services:
   ````shell
   docker-compose down
   unset COMPOSE_FILE
   # Remaining containers:
   docker stop $(docker container ls -qa)
   docker rm $(docker container ls -qa)
   ````
### Automated Test
1. Single RabbitMQ partition:
   ````shell
   unset COMPOSE_FILE
   ./test_them_all.sh start stop
   ````
2. Run the tests for RabbitMQ with two partitions per topic using the Docker Compose docker-compose-partitions.yml file with the following commands:
   ````shell
   export COMPOSE_FILE=docker-compose-partitions.yml 
   ./test_them_all.sh start stop
   unset COMPOSE_FILE
   ````
3. Run test with Kafka and 4 Partitions per Topic
   ````shell
   export COMPOSE_FILE=docker-compose-kafka.yml 
   ./test_them_all.sh start stop
   unset COMPOSE_FILE
   ````

